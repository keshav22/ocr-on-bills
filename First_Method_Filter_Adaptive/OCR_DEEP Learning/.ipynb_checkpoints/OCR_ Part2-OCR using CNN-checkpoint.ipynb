{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HqYb5NXVDdSN"
   },
   "source": [
    "# **Load Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "SOtEHEKq_hWe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import os\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bold = '\\033[1m'\n",
    "end = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mpV8eHhLD3V3"
   },
   "source": [
    "# **Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "EfTGyBIhB0DO"
   },
   "outputs": [],
   "source": [
    "def read_data(file_list):\n",
    "  '''\n",
    "  read data from tfrecords file\n",
    "  '''\n",
    "  file_queue=tf.train.string_input_producer(file_list)\n",
    "  feature = {'images': tf.FixedLenFeature([], tf.string),\n",
    "             'labels': tf.FixedLenFeature([], tf.string)}    \n",
    "  reader = tf.TFRecordReader()  \n",
    "  _,record=reader.read(file_queue)\n",
    "  features = tf.parse_single_example(record, features=feature)\n",
    "  img = tf.decode_raw(features['images'], tf.uint8)\n",
    "  label = tf.decode_raw(features['labels'], tf.uint8)\t\n",
    "  return img,label\n",
    "\n",
    "\n",
    "\n",
    "def minibatch(folder_path, filename, file_count, \n",
    "              image_size, max_char, class_count, \n",
    "              batch_size,):\n",
    "  '''\n",
    "  generate minibatch\n",
    "  '''\n",
    "  file_list = [os.path.join(folder_path, filename + '%d.tfrecords' % i) for i in range(1, file_count+1)]  \n",
    "  img, label = read_data(file_list)\n",
    "  img = tf.cast(tf.reshape(img,img_size), dtype = tf.float32)\n",
    "  label = tf.reshape(label, [1, max_char])\n",
    "  label = tf.one_hot(label,class_count,axis=1)\n",
    "  label = tf.reshape(label,tf.shape(label)[1:])\n",
    "  img_batch,label_batch= tf.train.shuffle_batch([img, label],batch_size,capacity,min_after_dequeue,num_threads=num_of_threads)\n",
    "  return img_batch, tf.cast(label_batch, dtype = tf.int64)\n",
    "\n",
    "\n",
    "\n",
    "def variable(name,shape,initializer,weight_decay = None):\n",
    "  '''\n",
    "  create parameter tensor\n",
    "  '''\n",
    "  var = tf.get_variable(name, shape, initializer = initializer)\n",
    "  if weight_decay is not None:\n",
    "    weight_loss=tf.multiply(tf.nn.l2_loss(var),weight_decay,name=\"weight_loss\")\n",
    "    tf.add_to_collection('losses', weight_loss)\n",
    "  return var\n",
    "\n",
    "\n",
    "\n",
    "def conv_block(block_num,\n",
    "               input_data,\n",
    "               weights, \n",
    "               weight_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "               bias_initializer=tf.constant_initializer(0.0),\n",
    "               conv_op=[1,1,1,1],\n",
    "               conv_padding='SAME',\n",
    "               weight_decay=None,\n",
    "               lrn=True,\n",
    "               dropout=1.0, \n",
    "               activation=True):\n",
    "  '''\n",
    "  convolutional block\n",
    "  '''\n",
    "  with tf.variable_scope('conv'+ str(block_num), reuse = tf.AUTO_REUSE) as scope:\n",
    "    input_data = tf.nn.dropout(input_data, dropout)\n",
    "    kernel = variable('weights', weights, initializer = weight_initializer, weight_decay = weight_decay)\n",
    "    biases = variable('biases', weights[3], initializer=bias_initializer, weight_decay=None)\n",
    "    conv = tf.nn.conv2d(input_data, kernel, conv_op, padding=conv_padding)\n",
    "    pre_activation = tf.nn.bias_add(conv, biases)\n",
    "    if lrn==True:\n",
    "      pre_activation = tf.nn.lrn(pre_activation, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,name='norm')\n",
    "    if activation:\n",
    "      conv_out = tf.nn.relu(pre_activation, name=scope.name)\n",
    "      return conv_out\n",
    "    else:\n",
    "      return pre_activation\n",
    "\n",
    "\n",
    "\n",
    "def dense_block(block_num,\n",
    "                input_data,\n",
    "                neurons,\n",
    "                weight_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                bias_initializer=tf.constant_initializer(0.0),\n",
    "                weight_decay=None,\n",
    "                activation=True, \n",
    "                dropout=1.0):\n",
    "  '''\n",
    "  Fully connected block\n",
    "  '''\n",
    "  with tf.variable_scope('dense'+ str(block_num), reuse = tf.AUTO_REUSE) as scope:\n",
    "    input_data = tf.nn.dropout(input_data, dropout)\n",
    "    weights = variable('weights', [input_data.shape[1], neurons], initializer=weight_initializer, weight_decay = weight_decay)\n",
    "    biases = variable('biases', [1,neurons], initializer = bias_initializer, weight_decay = None)\n",
    "    dense = tf.matmul(input_data,weights)+biases\n",
    "    if activation:\n",
    "      dense=tf.nn.relu(dense, name=scope.name)\n",
    "    return dense\n",
    "  \n",
    "  \n",
    "  \n",
    "def multi_loss(logits, labels, batch_size, max_char):\n",
    "  '''\n",
    "  cross entopy loss for multi class\n",
    "  '''\n",
    "  loss = 0\n",
    "  for i in range(max_char):\n",
    "    loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits\\\n",
    "            (logits=logits[:,i,:],labels=labels[:,:,i]), \\\n",
    "                           name='cross_entropy_loss_mean')\n",
    "  loss /= max_char\n",
    "  tf.add_to_collection('losses', loss)\n",
    "  total_loss=tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "  tf.add_to_collection('losses', total_loss)\n",
    "  return total_loss\n",
    "\n",
    "\n",
    "  \n",
    "def parameter_update(loss, learning_rate):\n",
    "  '''\n",
    "  optimization and parameter update using adam optimizer\n",
    "  '''\n",
    "  optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "  for var in tf.trainable_variables():\n",
    "    tf.summary.histogram(var.op.name, var)\n",
    "  return optimizer\n",
    "\n",
    "\n",
    "\n",
    "def accuracy_calc(output, label_batch):\n",
    "  '''\n",
    "  calculate accuracy\n",
    "  '''\n",
    "  correct_prediction = tf.equal(tf.cast(tf.argmax(output, 2),dtype=tf.int32),tf.cast(tf.argmax(label_batch, 1),dtype=tf.int32))\n",
    "  accuracy=tf.reduce_mean(tf.cast(correct_prediction,\"float\"))\n",
    "  return accuracy\n",
    "\n",
    "\n",
    "\n",
    "#evaluation\n",
    "def eval(batch_size=1000):\n",
    "\tsteps=((test_data_count))//batch_size\n",
    "\taccu=0\n",
    "\tx_test, y_test = minibatch(folder_path, test_filename, file_count, img_size, max_char, class_count,batch_size)\n",
    "\tlogit_test = inference(x_test, class_count)\n",
    "\taccuracy_test = accuracy_calc(logit_test, y_test)\n",
    "  \n",
    "\tinit=tf.global_variables_initializer()\n",
    "\tsaver=tf.train.Saver()\n",
    "\twith tf.Session() as sess:\n",
    "\t\tsess.run(init)\n",
    "\t\tcoord = tf.train.Coordinator()\n",
    "\t\tthreads = tf.train.start_queue_runners(coord=coord)\n",
    "\t\t#saver.restore(sess,checkpoint_restore)\n",
    "\t\tfor s in range(steps):\n",
    "\t\t\tacc=sess.run(accuracy_test)\n",
    "\t\t\taccu+=acc/steps\n",
    "\t\tprint(\"test set accuracy: \",acc)\n",
    "\n",
    "\t\tcoord.request_stop()\n",
    "\t\tcoord.join(threads)\n",
    "\n",
    "\t\treturn None\n",
    "  \n",
    "  \n",
    "def decoding(encoded_data, type = 'logit'):\n",
    "  if(type == 'logit'):\n",
    "    prediction = np.argmax(encoded_data, 2)\n",
    "  elif(type == 'label'):\n",
    "    prediction = np.argmax(encoded_data, 1)\n",
    "  decoded_prediction = []\n",
    "  for dp in prediction:\n",
    "    predicted_text = ''\n",
    "    for p in dp:\n",
    "      predicted_text += all_chr[p]\n",
    "    decoded_prediction.append(predicted_text)\n",
    "  return decoded_prediction\n",
    "\n",
    "\n",
    "\n",
    "def eval_vizualization(X):\n",
    "  decoded_text = []\n",
    "  logit = inference(X, class_count)\n",
    "  init=tf.global_variables_initializer()\n",
    "  saver=tf.train.Saver()\n",
    "  \n",
    "  with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    #saver.restore(sess,checkpoint_restore)\n",
    "    text = sess.run(logit)\n",
    "    decoded_text = decoding(text, type = 'logit')\n",
    "  for i in range(X.shape[0]):\n",
    "    x = np.reshape(X[i, :,:,:], img_size[0:2])\n",
    "    plt.imshow(x, cmap = 'gray')\n",
    "    plt.show()\n",
    "    print(\"text: \", decoded_text[i], '<---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q1gISaBjD9Yf"
   },
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Dx4ssU97CMm4"
   },
   "outputs": [],
   "source": [
    "def inference(image_batch, class_count,\n",
    "              dropout=[1,1,1,1],\n",
    "              wd=None):\n",
    "  '''\n",
    "  Forward propagation\n",
    "  '''\n",
    "\n",
    "  i = 0\n",
    "  weights=[[3,3,1,class_count//4],\n",
    "           [3,3,class_count//4,class_count//2],\n",
    "           [3,3,class_count//2,class_count],\n",
    "           [3,3,class_count,class_count]]\n",
    "  conv_op=[[1,1,1,1],[1,1,1,1],[1,1,1,1], [1,1,1,1]]\n",
    "  \n",
    "  conv1 = conv_block(1,image_batch,weights[i], conv_op = conv_op[i], conv_padding='SAME', dropout=dropout[i],weight_decay=wd)\n",
    "  i=i+1\n",
    "  pool1=tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1,2,2,1],padding='VALID', name='pool1') #16x128\n",
    "  \n",
    "  conv2 = conv_block(2,pool1,weights[i], conv_op = conv_op[i], conv_padding='SAME', dropout=dropout[i],weight_decay=wd)\n",
    "  i=i+1\n",
    "  pool2=tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1,2,2,1],padding='VALID', name='pool2') #8x64\n",
    "  \n",
    "  conv3 = conv_block(3,pool2,weights[i], conv_op = conv_op[i], conv_padding='SAME', dropout=dropout[i],weight_decay=wd)\n",
    "  i=i+1\n",
    "  pool3=tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides=[1,2,2,1],padding='VALID', name='pool3') #4x32\n",
    "  \n",
    "  conv4 = conv_block(4,pool3,weights[i], conv_op = conv_op[i], conv_padding='SAME', dropout=dropout[i],weight_decay=wd)\n",
    "  pool4=tf.nn.max_pool(conv4, ksize=[1, 4, 2, 1], strides=[1,1,2,1],padding='VALID', name='pool4') #1x16\n",
    "  \n",
    "  flat=tf.reshape(pool4, [tf.shape(image_batch)[0],max_char, class_count], name='flat')\n",
    "\t\t\n",
    "  return flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "49g9QIAXEBWj"
   },
   "source": [
    "# **Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VVGIHADUCVJD"
   },
   "outputs": [],
   "source": [
    "def train(folder_path, train_filename, test_filename, \n",
    "          data_per_train_file, test_data_count, file_count,\n",
    "          weights, dropout, wd,\n",
    "          img_size, max_char, class_count, \n",
    "          batch_size = 32, learning_rate=0.01, epochs=5, \n",
    "          restore=False, var_lr = [None,None]):\n",
    "  \n",
    "\ttrain_step = train_data_count//batch_size\n",
    "\ttest_step = test_data_count//batch_size\n",
    "  \n",
    "  #build graph\n",
    "\twith tf.Graph().as_default():\n",
    "\t\tx_train, y_train = minibatch(folder_path, train_filename, file_count, img_size, max_char, class_count,batch_size)     \n",
    "\t\tlogit_train = inference(x_train, class_count, dropout = dropout, wd = wd)\n",
    "\t\tcost = multi_loss(logit_train, y_train, batch_size, max_char)\n",
    "\t\tupdate=parameter_update(cost,learning_rate)\t\n",
    "\t\taccuracy_train = accuracy_calc(logit_train, y_train)\n",
    "    \n",
    "\t\tx_test, y_test = minibatch(folder_path, test_filename, file_count, img_size, max_char, class_count,batch_size)\n",
    "\t\tlogit_test = inference(x_test, class_count)\n",
    "\t\taccuracy_test = accuracy_calc(logit_test, y_test)\n",
    "    \n",
    "\t\tsaver = tf.train.Saver()\n",
    "    \n",
    "    #start session\n",
    "\t\twith tf.Session() as sess:\n",
    "      #initialize the variables\n",
    "\t\t\tsess.run(tf.global_variables_initializer())\n",
    "\t\t\tsess.run(tf.local_variables_initializer())\n",
    "\t\t\tcoord = tf.train.Coordinator()\n",
    "\t\t\tthreads = tf.train.start_queue_runners(coord=coord) \n",
    "      \n",
    "      #restore the variables\n",
    "\t\t\tif restore == True:\n",
    "\t\t\t\tloader = tf.train.import_meta_graph(checkpoint_restore +'.meta')\n",
    "\t\t\t\tloader.restore(sess, checkpoint_restore)\n",
    "        \n",
    "\t\t\t#train for given number of epochs\n",
    "\t\t\tfor e in range(epochs): \n",
    "\t\t\t\tprint(bold + \"\\nepoch:\" + end, e)\n",
    "\t\t\t\ttrain_epoch_cost = 0\n",
    "\t\t\t\ttrain_epoch_acc = 0\n",
    "\t\t\t\ttest_epoch_acc = 0\n",
    "        \n",
    "        #train for given number of steps in one epoch\n",
    "\t\t\t\tfor s in range(train_step):\n",
    "\t\t\t\t\t_,train_batch_cost = sess.run([update, cost])\t          \n",
    "\t\t\t\t\tif s % (train_step//20) == 0 and s != 0:\n",
    "\t\t\t\t\t\tprint('~', end = '')\n",
    "\t\t\t\t\telif(s == (train_step) - 1):\n",
    "\t\t\t\t\t\tprint('')            \n",
    "\t\t\t\t\ttrain_epoch_cost += train_batch_cost/(train_step)\t          \n",
    "\t\t\t\tprint(bold + \"epoch_cost: \" + end,train_epoch_cost)\n",
    "        \n",
    "        #calculate accuracy of training set\n",
    "\t\t\t\tfor i in range(train_step//5):\n",
    "\t\t\t\t\ttrain_epoch_acc = sess.run(accuracy_train)\n",
    "\t\t\t\t\ttrain_epoch_acc += train_epoch_acc/(train_step)        \n",
    "\t\t\t\tprint(bold + \"train epoch accuracy: \" + end,train_epoch_acc, \"\\n\")\n",
    "        \n",
    "        #calculate accuracy of test set\n",
    "\t\t\t\tfor i in range(test_step):\n",
    "\t\t\t\t\ttest_epoch_acc = sess.run(accuracy_test)\n",
    "\t\t\t\t\ttest_epoch_acc += test_epoch_acc/test_step    \n",
    "\t\t\t\tprint(bold + \"test epoch accuracy: \" + end, test_epoch_acc, \"\\n\")\n",
    "        \n",
    "        #afer every 20 epoch decrease learning rate by factor of 10\n",
    "\t\t\t\tif var_lr[0] != None:\n",
    "\t\t\t\t\tif e%var_lr[0] == 0:\n",
    "\t\t\t\t\t\tlearning_rate = learning_rate/var_lr[1]\n",
    "      \n",
    "      #save all the variables\n",
    "\t\t\tsave_path = saver.save(sess, checkpoint_save)\t\n",
    "\t\t\tcoord.request_stop()\n",
    "\t\t\tcoord.join(threads)\n",
    "\t\t\t\n",
    "\treturn None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "y6J1kZAECcXE"
   },
   "outputs": [],
   "source": [
    "folder_path = \"./tfrecordoutput/\"\n",
    "file_count = 2\n",
    "train_data_count = 8192 * file_count\n",
    "test_data_count = 2048 * file_count\n",
    "img_size = [32,256,1]\n",
    "class_count = 63\n",
    "max_char = 16\n",
    "keyword = '3to8'\n",
    "train_filename = 'train_' + keyword + '_'\n",
    "test_filename = 'test_' + keyword + '_'\n",
    "checkpoint_restore = \"checkpoint_3to8.ckpt\"\n",
    "checkpoint_save = \"checkpoint_3to8.ckpt\"\n",
    "\n",
    "dropout = [1, 1, 1, 1]\n",
    "wd = 0.000\n",
    "lr = 0.01\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "num_of_threads=16\n",
    "min_after_dequeue=5000\n",
    "capacity=min_after_dequeue+(num_of_threads+1)*batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 434949,
     "status": "ok",
     "timestamp": 1532018515949,
     "user": {
      "displayName": "Vijendra Singh",
      "photoUrl": "//lh3.googleusercontent.com/-NdZ5dixuTHU/AAAAAAAAAAI/AAAAAAAArI4/R6qZstWMpR4/s50-c-k-no/photo.jpg",
      "userId": "111198229299491674196"
     },
     "user_tz": -330
    },
    "id": "uXtfaRdjDOBK",
    "outputId": "21d63f3b-80dd-4ed2-d51a-cd5e63e2cbfe",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-a2b9923b4798>:104: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "\u001b[1m\n",
      "epoch:\u001b[0m 0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "\u001b[1mepoch_cost: \u001b[0m 1.2440493390895426\n",
      "\u001b[1mtrain epoch accuracy: \u001b[0m 0.8473548889160156 \n",
      "\n",
      "\u001b[1mtest epoch accuracy: \u001b[0m 0.84246826171875 \n",
      "\n",
      "\u001b[1m\n",
      "epoch:\u001b[0m 1\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "\u001b[1mepoch_cost: \u001b[0m 0.668015128467232\n",
      "\u001b[1mtrain epoch accuracy: \u001b[0m 0.8512687683105469 \n",
      "\n",
      "\u001b[1mtest epoch accuracy: \u001b[0m 0.8345947265625 \n",
      "\n",
      "\u001b[1m\n",
      "epoch:\u001b[0m 2\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "\u001b[1mepoch_cost: \u001b[0m 0.5868764946353622\n",
      "\u001b[1mtrain epoch accuracy: \u001b[0m 0.8532257080078125 \n",
      "\n",
      "\u001b[1mtest epoch accuracy: \u001b[0m 0.8523101806640625 \n",
      "\n",
      "\u001b[1m\n",
      "epoch:\u001b[0m 3\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "\u001b[1mepoch_cost: \u001b[0m 0.5572655341820791\n",
      "\u001b[1mtrain epoch accuracy: \u001b[0m 0.8806228637695312 \n",
      "\n",
      "\u001b[1mtest epoch accuracy: \u001b[0m 0.9113616943359375 \n",
      "\n",
      "\u001b[1m\n",
      "epoch:\u001b[0m 4\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "\u001b[1mepoch_cost: \u001b[0m 0.5260253489250317\n",
      "\u001b[1mtrain epoch accuracy: \u001b[0m 0.8786659240722656 \n",
      "\n",
      "\u001b[1mtest epoch accuracy: \u001b[0m 0.9271087646484375 \n",
      "\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Enqueue operation was cancelled\n",
      "\t [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueueV2[Tcomponents=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](shuffle_batch/random_shuffle_queue, Cast, Reshape_2)]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Parent directory of checkpoint_3to8.ckpt doesn't exist, can't save.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Failed to create a directory: ; No such file or directory\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, beta1_power, beta2_power, conv1/biases, conv1/biases/Adam, conv1/biases/Adam_1, conv1/weights, conv1/weights/Adam, conv1/weights/Adam_1, conv2/biases, conv2/biases/Adam, conv2/biases/Adam_1, conv2/weights, conv2/weights/Adam, conv2/weights/Adam_1, conv3/biases, conv3/biases/Adam, conv3/biases/Adam_1, conv3/weights, conv3/weights/Adam, conv3/weights/Adam_1, conv4/biases, conv4/biases/Adam, conv4/biases/Adam_1, conv4/weights, conv4/weights/Adam, conv4/weights/Adam_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[0;32m   1702\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1703\u001b[1;33m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[0;32m   1704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1334\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Failed to create a directory: ; No such file or directory\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, beta1_power, beta2_power, conv1/biases, conv1/biases/Adam, conv1/biases/Adam_1, conv1/weights, conv1/weights/Adam, conv1/weights/Adam_1, conv2/biases, conv2/biases/Adam, conv2/biases/Adam_1, conv2/weights, conv2/weights/Adam, conv2/weights/Adam_1, conv3/biases, conv3/biases/Adam, conv3/biases/Adam_1, conv3/weights, conv3/weights/Adam, conv3/weights/Adam_1, conv4/biases, conv4/biases/Adam, conv4/biases/Adam_1, conv4/weights, conv4/weights/Adam, conv4/weights/Adam_1)]]\n\nCaused by op 'save/SaveV2', defined at:\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2856, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-490afb6a1edb>\", line 6, in <module>\n    restore=False, var_lr=[None,None])\n  File \"<ipython-input-5-a61447ca5d7f>\", line 23, in train\n    saver = tf.train.Saver()\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1338, in __init__\n    self.build()\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1347, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1384, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 832, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 350, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 266, in save_op\n    tensors)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1800, in save_v2\n    shape_and_slices=shape_and_slices, tensors=tensors, name=name)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Failed to create a directory: ; No such file or directory\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, beta1_power, beta2_power, conv1/biases, conv1/biases/Adam, conv1/biases/Adam_1, conv1/weights, conv1/weights/Adam, conv1/weights/Adam_1, conv2/biases, conv2/biases/Adam, conv2/biases/Adam_1, conv2/weights, conv2/weights/Adam, conv2/weights/Adam_1, conv3/biases, conv3/biases/Adam, conv3/biases/Adam_1, conv3/weights, conv3/weights/Adam, conv3/weights/Adam_1, conv4/biases, conv4/biases/Adam, conv4/biases/Adam_1, conv4/weights, conv4/weights/Adam, conv4/weights/Adam_1)]]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-490afb6a1edb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m       \u001b[0mimg_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_char\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m       \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m       restore=False, var_lr=[None,None])\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-a61447ca5d7f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(folder_path, train_filename, test_filename, data_per_train_file, test_data_count, file_count, weights, dropout, wd, img_size, max_char, class_count, batch_size, learning_rate, epochs, restore, var_lr)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m       \u001b[1;31m#save all the variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m                         \u001b[0msave_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_save\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m                         \u001b[0mcoord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                         \u001b[0mcoord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthreads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[0;32m   1718\u001b[0m               \"Parent directory of {} doesn't exist, can't save.\".format(\n\u001b[0;32m   1719\u001b[0m                   save_path))\n\u001b[1;32m-> 1720\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1722\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwrite_meta_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Parent directory of checkpoint_3to8.ckpt doesn't exist, can't save."
     ]
    }
   ],
   "source": [
    "train(folder_path, train_filename, test_filename, \n",
    "      train_data_count, test_data_count, file_count,\n",
    "      class_count, dropout, wd,\n",
    "      img_size, max_char, class_count, \n",
    "      batch_size=batch_size, learning_rate=lr, epochs=epochs, \n",
    "      restore=False, var_lr=[None,None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GcsdqO1cElm3"
   },
   "source": [
    "# **Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "zjvFGVgnWO7H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set accuracy:  0.0055\n"
     ]
    }
   ],
   "source": [
    "eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 52902,
     "status": "ok",
     "timestamp": 1532017545919,
     "user": {
      "displayName": "Vijendra Singh",
      "photoUrl": "//lh3.googleusercontent.com/-NdZ5dixuTHU/AAAAAAAAAAI/AAAAAAAArI4/R6qZstWMpR4/s50-c-k-no/photo.jpg",
      "userId": "111198229299491674196"
     },
     "user_tz": -330
    },
    "id": "afLrhQHKDPcw",
    "outputId": "419dbe6c-ed4b-4e89-d48e-47f8355be6f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoint_3to8.ckpt\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Enqueue operation was cancelled\n",
      "\t [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueueV2[Tcomponents=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](shuffle_batch/random_shuffle_queue, Cast, Reshape_2)]]\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoint_3to8.ckpt\n\t [[Node: save_4/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_4/Const_0_0, save_4/RestoreV2/tensor_names, save_4/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save_4/RestoreV2', defined at:\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-5b617e55c252>\", line 12, in <module>\n    eval_vizualization(x_c[1:5])\n  File \"<ipython-input-10-ab10fbbe26f7>\", line 179, in eval_vizualization\n    saver=tf.train.Saver()\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1338, in __init__\n    self.build()\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1347, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1384, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 835, in _build_internal\n    restore_sequentially, reshape)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 472, in _AddRestoreOps\n    restore_sequentially)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 886, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1546, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoint_3to8.ckpt\n\t [[Node: save_4/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_4/Const_0_0, save_4/RestoreV2/tensor_names, save_4/RestoreV2/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoint_3to8.ckpt\n\t [[Node: save_4/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_4/Const_0_0, save_4/RestoreV2/tensor_names, save_4/RestoreV2/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-5b617e55c252>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m   \u001b[0mx_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_check\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m   \u001b[0meval_vizualization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_c\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m   \u001b[0mcoord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-ab10fbbe26f7>\u001b[0m in \u001b[0;36meval_vizualization\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m    181\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcheckpoint_restore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[0mdecoded_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'logit'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1800\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1801\u001b[0m       sess.run(self.saver_def.restore_op_name,\n\u001b[1;32m-> 1802\u001b[1;33m                {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[0;32m   1803\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1804\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoint_3to8.ckpt\n\t [[Node: save_4/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_4/Const_0_0, save_4/RestoreV2/tensor_names, save_4/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save_4/RestoreV2', defined at:\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-5b617e55c252>\", line 12, in <module>\n    eval_vizualization(x_c[1:5])\n  File \"<ipython-input-10-ab10fbbe26f7>\", line 179, in eval_vizualization\n    saver=tf.train.Saver()\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1338, in __init__\n    self.build()\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1347, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1384, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 835, in _build_internal\n    restore_sequentially, reshape)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 472, in _AddRestoreOps\n    restore_sequentially)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 886, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1546, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\kesha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoint_3to8.ckpt\n\t [[Node: save_4/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_4/Const_0_0, save_4/RestoreV2/tensor_names, save_4/RestoreV2/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "all_chr = list(string.ascii_letters) + list(string.digits) + list(' ')\n",
    "x_check, y_check=minibatch(folder_path, train_filename, file_count, img_size, max_char, class_count,batch_size) \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  sess.run(tf.local_variables_initializer())\n",
    "  coord = tf.train.Coordinator()\n",
    "  threads = tf.train.start_queue_runners(coord=coord) \n",
    "  \n",
    "  x_c = sess.run(x_check)  \n",
    "  eval_vizualization(x_c[1:5])\n",
    "  \n",
    "  coord.request_stop()\n",
    "  coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Zq-1-sdKEruW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "OCR using CNN.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
